% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.
\chapter{Prelimiaries}\label{chapter:prelimiaries}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}
\newtheorem{example}{Example}[section]

In this section, we introduce the notation used throughout the paper, and capture the most important preliminaries in definitions. We start with the mathematical notation and concepts used in this paper.

\section{Mathematical Prelimiaries}

We let $p$ and $q$ denote prime numbers if not explicitly stated otherwise. 
Groups are written in a multiplicate manner with the  $\cdot$ operator and the abbreviation $ab$ for $a \cdot b$. We let $g$ and $h$ denote group elements if not explicitly stated otherwise.
Furthermore, we use the notation $\mathbb{F}_p$ for a finite field of prime order p (note that the integers modulo p are isomorph to any finite field of prime order p \parencite{algebra}) with the conventional operators `+` and `$\cdot$` for addition and multiplication. We let $a$,$b$, and $c$ denote finite field elements if not explicitly stated otherwise.

\begin{definition}[cyclic group]
Let $\mathcal{G}$ be a group of prime order p. We call a group cyclic iff: $\exists g \in \mathcal{G}. \ \forall e \in \mathcal{G}. \ \exists n \in \mathbb{N}. \ e = g^n$, which is equivalent to $\mathcal{G} = \{1,g,g^2,...,g^{p-1}\}$ \parencite{algebra}. If such a $g$ exists, we call it a generator.

From now on we write \textbf{g} for a randomly chosen but fixed generator of a respective cyclic group. 
\end{definition}

\begin{definition}[pairings]
    \label{pairings_def}
    Let $\mathcal{G}$ and $\mathcal{H}$ be two groups of prime order p. A pairing is a function: $\mathcal{G} \times \mathcal{G} \rightarrow \mathcal{H}$, with the following two properties:
    \begin{itemize}
        \item \textbf{Bilineraity:} $\forall g,h \in \mathcal{G}. \ \forall a,b \in \mathbb{F}_p. \ e(g^a,h^b) = e(g,h)^{ab}$
        \item \textbf{Non-degeneracy:} $\neg (\forall g,h \in \mathcal{G}. \ e(g,h)=1)$
    \end{itemize}
    \parencite{KZG}
\end{definition}

From now on let $e$ denote a pairing function if not explicitly stated otherwise.

Now that we have introduced the mathematical preliminaries we will tend to the cryptographic preliminaries. 

\section{Cryptographic Prelimiaries}
In this section, we will introduce the security notions that we use in this paper and the concepts behind them.

We start with the definition of a negligable and a poly-bounded function from which we will define our adversarial model, against which we will prove security in this paper.

\begin{definition}
    Let $f: \mathbb{Z}_{\ge 0} \rightarrow \mathbb{R}$ be a function. We call $f$ negligable iff:
    \begin{equation*}
        \forall c \in \mathbb{R}_{> 0}. \ \exists n_0. \ \forall n \ge n_0. \ \vert f(n)\vert < 1/n^c
    \end{equation*}
    \parencite{boneh_shoup}
\end{definition}
Boneh and Shoup state "Intuitively, a negligible function $f:\mathbb{F}_{\ge 0} \rightarrow \mathbb{R}$ is one that not only tends to zero as $n \rightarrow \infty$, but
does so faster than the inverse of any polynomial." \parencite{boneh_shoup}

From now on let $\epsilon$ denote a negligible function if not explicitly stated otherwise. 

\begin{definition}
    Let $f: \mathbb{Z}_{\ge 0} \rightarrow \mathbb{R}$ be a function. We call $f$ poly-bounded iff:
    \begin{equation*}
        \exists c,d \in \mathbb{R}_{>0}. \ \forall n \in \mathbb{N}_0. \ \vert f(n)\vert \le n^c+d
    \end{equation*}
    \parencite{boneh_shoup}
\end{definition}


Note, we will define (probabilistic) algorithms for some security parameter $\kappa$ and bound their performance using the notion of negligibility and poly-boundedness with respect to $\kappa$.

We capture the security of our cryptographic system in games against an (efficient) adversary. Typically, the adversary has to break a security property in those games (e.g. decrypt a cyphertext). However, before we formally define games, we define what an Adversary is.

\begin{definition}[efficient Adversary]
    \label{Adversary}
An Adversary is a probabilistic algorithm, that takes a security parameter $\kappa$ as its first argument and returns some probabilistic result. 
We call an Adversary efficient if its running time is poly-bounded in $\kappa$ except for negligible probability (with respect to $\kappa$)
\parencite{boneh_shoup}.
\end{definition}

Besides this definition, we will use a stronger definition of Adversaries, namely that of the Adversary in the Algebraic Group Model (AGM) \parencite{AGM}.

\begin{definition}[AGM Adversary]
    Let $\mathbb{F}_p$ be a finite field of prime order p and $\mathbb{G}$ a cyclic group of prime order p. An adversary in the AGM is an adversary as in definition \ref{Adversary}, that furthermore outputs a vector $\vec{z} \in \mathbb{F}_p^t$ for every element $e$ from $\mathbb{G}$ in its output, such that $e = \prod_{1}^{t} g_i^{z_i}$, where $g\in \mathbb{G}^t$ is the vector of all elements of $\mathbb{G}$ that the Adversary has seen so far
    \parencite{AGM}. 

    The efficiency definition is analogue to definition \ref*{Adversary}
\end{definition}

Now that we have defined adversary models, we define games. 

\begin{definition}[games]
Games are probabilistic algorithms with access to an Adversary and output a boolean value \parencite{boneh_shoup}. Formally we write games as a sequence of functions and Adversary calls \parencite{boneh_shoup}.
\end{definition}

Notatioinwise we write $`\leftarrow`$ followed by a set for uniform sampling from that set, $`\leftarrow`$ followed by a probability mass function (e.g. an Adversary result) to sample from that function space, and $`=`$ for an assignment of a deterministic value. Moreover, we write $`:`$ followed by a condition to assure that the condition has to hold at this point. To give an example, think of the following game as "sampling a uniformly random $a$ from $\mathbb{F}_p$, get the probabilistic result from $\mathcal{A}$ as $b$, computing $c$ as $F$ applied to $a$ and $b$, and assert that $P$ holds for $c$":
\begin{equation*}
    \left(
    \begin{aligned}
        a & \leftarrow \mathbb{F}_p, \\
        b & \leftarrow \mathcal{A}, \\
        c & = F(a,b) \\
        & : \ P(c)
    \end{aligned}
    \right)
\end{equation*}

Next, we define game-based proofs, the method which we will use to formally prove security.

\begin{definition}[game-based proofs]
    Game-based proofs are a sequence of game-hops that bound the probability of one game to another \parencite{gamesB&R,shoup_games}.

    The two types of game hops we will use in our proofs are:
    \begin{itemize}
        \item \textbf{game hop as a bridging step:} 
        
        A bridging step alters the function definitions, such that the game probability does not change \parencite{shoup_games}.
        \item \textbf{game hop based on a failure event:}
        
        In a game hop based on a failure event, two games are equal except if a specific failure event occurs \parencite{shoup_games}. The failure event should have a negligible probability for the game-based proof to hold.
    \end{itemize}
\end{definition}

Typically we will define a game for a certain security definition applied to our cryptographic protocol and reduce that game using game hops to a hardness assumption game, thus showing that breaking the security definition for our cryptographic protocol is at least as hard as breaking the hardness assumption \parencite{boneh_shoup}. Hence we need to define hardness and accordingly hardness assumptions:

\begin{definition}[hardness]
    Given a computational problem P, we say P is hard if and only if no efficient adversary exists, that solves P with non-negligible probability \parencite{ac_handbook}. 
\end{definition}

\begin{definition}[hardness assumptions]
Hardness assumptions are computational problems that are generally believed to be hard \parencite{boneh_shoup,ac_handbook}.
\end{definition}

Within cryptography, there exist several hardness assumptions, we will cover the ones used in this paper (conveniently the KZG paper \parencite{KZG} already defines them) and formally define according games. 

From now on let `$\in_{\mathcal{R}}$` denote uniform sampling from the respective set. 

\begin{definition}[discrete logarithm (DL)]
    Let $\mathcal{G}$ be a cyclic group with generator \textbf{g}.
    For $a \in_{\mathcal{R}} \mathbb{F}_p$, holds for every Adversary $\mathcal{A}: \text{Pr}[a = \mathcal{A}(\textbf{g}^a)] = \epsilon$ \parencite{KZG}.

    Formally we define the DL game as: 
    \begin{equation*}
        \left(
            \begin{aligned}
                a & \leftarrow \mathbb{F}_p \\
                a' & \leftarrow \mathcal{A}(\textbf{g}^a) \\
                & : a = a'
            \end{aligned}
        \right)
    \end{equation*}
\end{definition}

\begin{definition}[t-Strong Diffie Hellmann (t-SDH)]
    Let $\mathcal{G}$ be a cyclic group with generator \textbf{g}.
    Let $t \in \mathbb{N}$ be fixed.  For $a \in_{\mathcal{R}} \mathbb{F}_p$, holds for every Adversary $\mathcal{A}:$
    \begin{equation*}
        \text{Pr}\big[
            (c,\textbf{g}^{\frac{1}{a+c}}) = \mathcal{A}([\textbf{g},\textbf{g}^a,\textbf{g}^{a^2},\dots, \textbf{g}^{a^{t-1}}])
        \big] = \epsilon
    \end{equation*}
    for all $c \in \mathbb{F}_p\backslash \{a\}$ \parencite{KZG}.
    
    Formally we define the t-SDH game as: 
    \begin{equation*}
        \left(
            \begin{aligned}
                a & \leftarrow \mathbb{F}_p \\
                (c,g') & \leftarrow \mathcal{A}([\textbf{g},\textbf{g}^a,\textbf{g}^{a^2},\dots, \textbf{g}^{a^{t-1}}]) \\
                & : \textbf{g}^{\frac{1}{a+c}} = g'
            \end{aligned}
        \right)
    \end{equation*}
\end{definition}

The following definition is analogous to the previous one, except that the result is passed through a pairing function. Nevertheless, we define the property formally for completeness.

\begin{definition}[t-Bilinear Strong Diffie Hellmann (t-BSDH)]
    Let $\mathcal{G}$ and $\mathcal{H}$ be cyclic groups with generators \textbf{g} and \textbf{h}.
    Let $t \in \mathbb{N}$ be fixed and $e: \mathcal{G} \times \mathcal{G} \rightarrow \mathcal{H}$ be a pairing function.  For $a \in_{\mathcal{R}} \mathbb{F}_p$, holds for every Adversary $\mathcal{A}:$
    \begin{equation*}
        \text{Pr}\big[
            (c,e(\textbf{g}, \textbf{g} )^{\frac{1}{a+c}}) = \mathcal{A}([\textbf{g},\textbf{g}^a,\textbf{g}^{a^2},\dots, \textbf{g}^{a^{t-1}}])
        \big] = \epsilon
    \end{equation*}
    for all $c \in \mathbb{F}_p\backslash \{a\}$ \parencite{KZG}.
    
    Formally we define the t-SDH game as: 
    \begin{equation*}
        \left(
            \begin{aligned}
                a & \leftarrow \mathbb{F}_p \\
                (c,g') & \leftarrow \mathcal{A}([\textbf{g},\textbf{g}^a,\textbf{g}^{a^2},\dots, \textbf{g}^{a^{t-1}}]) \\
                & : e(\textbf{g}, \textbf{g})^{\frac{1}{a+c}} = g'
            \end{aligned}
        \right)
    \end{equation*}
\end{definition}

Now that we have introduced the necessary preliminaries, notions, and definitions for proving security for cryptographic protocols, we tend to the type of protocols we formalize in this work.

\begin{definition}[Commitment Schemes (CS)]
    \label{CS}
    A Commitment Scheme is a cryptographic protocol between two parties, we call the committer and the verifier, and consists of three functions:
    \begin{itemize}
        \item \textbf{KeyGen} generates a key $ck$ for the commiter and a key $vk$ for the verifier.
        \item \textbf{Commit}
        takes the commiter key $ck$, a message $m$ and computes a commitment $C$ for $m$ and a opening value $ov$. 
        \item \textbf{Verify} 
        takes the verifier key $vk$, a commitment $C$, an opening value $ov$, a message $m$ and decides whether $C$ is a valid commitment to $m$ using $vk$ and $ov$.
    \end{itemize}
    \parencite{thalerbook}

    The protocol assumes that KeyGen was used to distribute the keys $ck$ and $vk$ accordingly to the committer and verifier, such that no party can learn the keys of the other party if they are to remain secret. 

    Once the keys are correctly distributed, the protocol follows three steps: 
    \begin{enumerate}
        \item the commiter uses their keys $ck$ to commit to a arbitrary message $m$, invoking Commit$(ck,m)$ from which they obtain a commitment $C$ to $m$ and an opening value $ov$ for the commitment. The commiter stores $ov$ and sends $C$ to the verifier.
        \item at a later point in time, the committer might decide to open the commitment $C$ they sent to the verifier. To open the commitment the committer sends the opening value $ov$ and the message $m$ to the verifier. 
        \item the verifier invokes Verify on the values it received from the committer ($C,m$ and $ov$) to decide whether $m$ is the message the commitment $C$ was computed for. 
    \end{enumerate}
    Once the keys are set up, the protocol may run arbitrary times and in parallel (i.e. the committer can commit to arbitrary many messages and reveal them independently).
    % TODO Grafik f√ºr CS hier
\end{definition}

In our formalization, we work with a specific type of commitment scheme, namely Polynomial Commitment Schemes (PCS): 

\begin{definition}[Polynomial Commitment Scheme (PCS)]
    \label{PCS_def}
    A Polynomial Commitment Scheme is a Commitment Scheme as defined in \ref*{CS}, with its message space restricted to polynomials (i.e. messages are polynomials). 
    
    Furthermore, a PCS supports two more functions to allow point-wise (i.e. partly) opening a commitment to a polynomial: 
    \begin{itemize}
        \item \textbf{CreateWitness}
        takes the commiter key $ck$, a polynomial $\phi$, a value $i$ and computes a witness $w$ for the point $(i, \phi(i))$. 
        \item \textbf{VerifyEval:} 
        takes the verifier keys $vk$, a commitment $C$, a point $(i, \phi(i))$, a witness $w$ and checks that the point is consistent with the commitment (i.e. the point is part of the polynomial that $C$ is a commitment to) using $w$ and $vk$.
    \end{itemize}
\end{definition}

Note that we omit the verifier keys in the following four definitions for readability, but generally assume the Adversaries to have access to the verifier keys. 

We formally define four security properties for a PCS:

\begin{definition}[Polynomial Binding]
    We say a PCS is polynomial binding if and only if the probability of any efficient adversary finding a commitment value $C$, opening values $ov$ and $ov'$ and polynomials $\phi$ and $\phi'$, such that:
    \begin{equation*}
        \text{Pr}\big[
            \text{Verify}(C,ov,\phi) \land \text{Verify}(C,ov',\phi')
        \big]
        = \epsilon
    \end{equation*}
    \parencite{KZG}
\end{definition}

\begin{definition}[Evaluation Binding]
    We say a PCS is evaluation binding if and only if the probability of any efficient adversary finding a commitment value $C$, two witnesses $w$ and $w'$ and two evaluations $\phi(i)$ and $\phi(i)'$ for any $i$, such that:
    \begin{equation*}
        \text{Pr}\big[
            \text{VerifyEval}(C,(i,\phi(i)), w) \land \text{VerifyEval}(C,(i,\phi(i)'), w')
        \big]
        = \epsilon
    \end{equation*}
    \parencite{KZG}
\end{definition}

\begin{definition}[Knowledge Soundness (AGM)]
    We say a PCS is knowledge sound in the AGM if and only if there exists an efficient extractor algorithm $E$ such that for every efficient Adversary $\mathcal{A}=(\mathcal{A}_1, \mathcal{A}_2)$ in the AGM the probability of winning the following game is negligible: 
    \begin{equation*}
        \left(
            \begin{aligned}
                (ck,vk) & \leftarrow \text{KeyGen} \\
                (C,\vec{v}, \sigma) & \leftarrow \mathcal{A}_1(ck)\\
                p & \leftarrow E(C,\vec{v}) \\
                (i, \phi(i), w) &\leftarrow \mathcal{A}_2(\sigma)\\
                & : \phi(i) \ne p(i) \land \\
                & \hspace{3.5mm} \text{VerifyEval}(vk, C,(i,\phi(i)),w)
            \end{aligned}
        \right)
    \end{equation*}
    \parencite{plonk}
\end{definition}

\begin{definition}[hiding]
    We say a PCS is hiding if and only if the probability of any efficient adversary finding an, to them, unknown point of the polynomial $\phi$ from the commitment $C:=\text{Commit}(ck,\phi)$ and deg$(\phi)-1$ points with witness  $(i,\phi(i),\text{CreateWitness}(ck, \phi, i))$ is negligible.
    \parencite{KZG}
\end{definition}

\section{Isabelle/HOL}
Now that we have covered the theoretical preliminaries, we introduce Isabelle/HOL\footnote{https://isabelle.in.tum.de/}, the interactive theorem prover we use to formalize our proofs. 
Note that we will use the abbreviation \textit{Isabelle} to refer to \textit{Isabelle/HOL} from now on. 

Isabelle is an interactive theorem prover for higher-order logic (HOL) that supports a minimal functional programming language \parencite{isabelle_manual}, as well as a large set of formalizations of mathematical areas such as Algebra and Analysis. The most essential formalizations are shipped with Isabelle in the \textit{HOL-library}, besides that there exists a large database of formalizations, the \textit{Archive of Formal Proofs (AFP)}\footnote{https://www.isa-afp.org}. We will use the finite field definition as well as the factorization algorithm for polynomials over finite fields from the Berlekamp-Zassenhaus\parencite{Berlekamp_Zassenhaus-AFP} AFP entry. Furthermore, we use the Lagrange Polynomial Interpolation algorithm from the AFP entry \textit{Polynomial Interpolation}\parencite{Polynomial_Interpolation-AFP}.
Apart from that, we use another AFP entry that is central to our proofs, the CryptHOL framework\parencite{CryptHOL-AFP}:

\subsection{CryptHOL}
CryptHOL is a framework for game-based proofs in Isabelle \parencite{CryptHOL_game_based}. It supports games in a monadic notation over sub-probability mass functions (spmfs), we use spmfs in games to compose the result of the game, which itself is a spmf. The notation is drawn from Haskell's do-notation, to give an example game
in the correct syntax:

\begin{example}
    \hspace{0mm}
    \begin{lstlisting}[language=isabelle]
        do {
            x $\leftarrow$ sample_uniform S;
            return_spmf x
        }
    \end{lstlisting}

    The \textit{do \{\dots\}} block captures the monad environment.
    \textit{sample\_uniform} of S is a uniformly distributed spmf over the set S. 
    We use $\leftarrow$ to bind the spmf to x in the monad, intuitively this means x is not a concrete element of S, but represents any element of S with respect to the probability distribution of the spmf (which in the example is uniform).
    We use \textit{return\_spmf} to, intuitively, return the spmf, that x represents, which is \textit{sample\_uniform} of S. Hence, the monad in this example is, in fact, equivalent to the spmf '\textit{sample\_uniform} of S'.
    More complex games can be created by composing more functions.
\end{example}

Additionally, since we are dealing with \textit{sub}-probability mass functions, 
there exists an unassigned probability mass (e.g. if the spmf is undefined for some event). 
In CryptHOL, the unassigned probability mass stands for an error in the game and can be handled in a \textit{TRY ELSE} block (i.e. TRY A ELSE B, captures semantically 'try to return A if A is an error, return B'). 
Introducing errors allows us to assert statements, specifically e.g. that 
the messages returned by the adversary are wellformed.
To give an example in Isabelle's notation: 
\begin{example}
    \hspace{0mm}
    \begin{lstlisting}[language=isabelle]
        TRY do {
            x $\leftarrow$ sample_uniform S;
            y $\leftarrow \mathcal{A}$ x;
            _::unit $\leftarrow$ assert_spmf(valid_msg y);
            return_spmf y
        } ELSE (return_spmf 0)
    \end{lstlisting}
\end{example}

As in the first game, an elementary event of S is bound to x. The Adversary $\mathcal{A}$, which itself is a spmf, applied to x, is bound to y (which is a dspd). However, this time, y is not an immediate result, instead, we use an assert to check that y, the result from the adversary, is valid. The assert\_spmf results in an unassigned probability mass if and only if the statement passed to it is false, otherwise it will result in unit probability mass. If the result is an unassigned probability mass, this will, monad-typically, propagate through to the monad's result, which will trigger the ELSE branch, which in this example, results in the probability distribution with only 0 (i.e. Pr$[i=0]=1$). 
If the result of the assert\_spmf is a unit probability mass, the result will be neglected in the remaining part of the monad, in our example, this would mean that 'return\_spmf y' would be the result of the game, which is the result of the Adversary applied to x (note for arbitrary $\mathcal{A}$ and x 
\lstinline[columns=fixed]{x $\leftarrow \mathcal{A}$; return_spmf x} is equivalent to simply \lstinline[columns=fixed]{A} in a monad).